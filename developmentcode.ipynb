{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytest\n",
    "import uptide\n",
    "import datetime\n",
    "import pytz\n",
    "from scipy.stats import linregress\n",
    "from matplotlib.dates import date2num\n",
    "import fnmatch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_fwf('data/1947ABE.txt', skiprows=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.read_csv('data/aberdeen/2000ABE.txt', header=9, delim_whitespace=True)\n",
    "# read in fixed-width file, skipping the header\n",
    "df1 = pd.read_fwf('data/1947ABE.txt', skiprows=9)\n",
    "\n",
    "# remove units row\n",
    "df1.drop(0, inplace=True)\n",
    "\n",
    "# rename sea level column\n",
    "df1.rename(columns={df1.columns[3]: 'Sea Level'}, inplace=True)\n",
    "\n",
    "# drop cycle column\n",
    "df1.drop(columns=df1.columns[0], inplace=True)\n",
    "\n",
    "# remove dodgy values\n",
    "df1.replace(to_replace=\".*T$\",value={'Sea Level':np.nan},regex=True,inplace=True)\n",
    "df1.replace(to_replace=\".*N$\",value={'Sea Level':np.nan},regex=True,inplace=True)\n",
    "df1.replace(to_replace=\".*M$\",value={'Sea Level':np.nan},regex=True,inplace=True)\n",
    "df1.replace(to_replace=\".*T$\",value={'Residual':np.nan},regex=True,inplace=True)\n",
    "df1.replace(to_replace=\".*N$\",value={'Residual':np.nan},regex=True,inplace=True)\n",
    "df1.replace(to_replace=\".*M$\",value={'Residual':np.nan},regex=True,inplace=True)\n",
    "\n",
    "# convert strings to numbers and datetimes\n",
    "df1['Date'] = pd.to_datetime(df1['Date'], format='%Y/%m/%d')\n",
    "df1['Time'] = pd.to_datetime(df1['Time'], format='%H:%M:%S') # or pd.to_timedelta(df1['Time'], unit='s')\n",
    "df1['Sea Level'] = df1['Sea Level'].astype('float')\n",
    "df1['Residual'] = df1['Residual'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df1['datetime'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tidal_data(filename):\n",
    "   # Reads in filename into a pandas dataframe which is cleaned and formatted. Returns the dataframe.\n",
    "   \n",
    "   # read in fixed-width file, skipping the header\n",
    "   df = pd.read_fwf(filename, skiprows=9)\n",
    "\n",
    "   # remove units row\n",
    "   df.drop(0, inplace=True)\n",
    "\n",
    "   # rename sea level column\n",
    "   df.rename(columns={df.columns[3]: 'Sea Level'}, inplace=True)\n",
    "\n",
    "   # drop cycle column\n",
    "   df.drop(columns=df.columns[0], inplace=True)\n",
    "\n",
    "   # remove dodgy values\n",
    "   df.replace(to_replace=\".*T$\",value={'Sea Level':np.nan},regex=True,inplace=True)\n",
    "   df.replace(to_replace=\".*N$\",value={'Sea Level':np.nan},regex=True,inplace=True)\n",
    "   df.replace(to_replace=\".*M$\",value={'Sea Level':np.nan},regex=True,inplace=True)\n",
    "   df.replace(to_replace=\".*T$\",value={'Residual':np.nan},regex=True,inplace=True)\n",
    "   df.replace(to_replace=\".*N$\",value={'Residual':np.nan},regex=True,inplace=True)\n",
    "   df.replace(to_replace=\".*M$\",value={'Residual':np.nan},regex=True,inplace=True)\n",
    "\n",
    "   # convert strings to numbers and datetimes\n",
    "   df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "   df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S') # or pd.to_timedelta(df['Time'], unit='s')\n",
    "   df['Sea Level'] = df['Sea Level'].astype('float')\n",
    "   df['Residual'] = df['Residual'].astype('float')\n",
    "\n",
    "   # create datetime column\n",
    "   df['datetime'] = df['Date'] + pd.to_timedelta(df['Time'].dt.time.astype(str))\n",
    "\n",
    "   # set datetime as index\n",
    "   df.set_index('datetime', inplace=True)\n",
    "\n",
    "   # NB don't drop Date and Time columns as they are used in tests\n",
    "\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_tidal_data(\"data/1947ABE.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"Sea Level\" in data.columns\n",
    "assert type(data.index) == pd.core.indexes.datetimes.DatetimeIndex\n",
    "assert data['Sea Level'].size == 8760\n",
    "assert '1947-01-01 00:00:00' in data.index\n",
    "assert '1947-12-31 23:00:00' in data.index\n",
    "\n",
    "# check for M, N and T data; should be NaN\n",
    "assert data['Sea Level'].isnull().any()\n",
    "assert pd.api.types.is_float_dtype(data['Sea Level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe next step is to use uptide module to work out tidal constuents but first have to remove np.nan values. Code that illustrates how to use the uptide module is found in the website: https://jhill1.github.io/SEPwC.github.io/tides_python.html so just have to follow that.\n",
    "\n",
    "Not sure if I need to refactor join_data so that it can handle more than two files - actually no because the skeleton function has arguments data1 and data2 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(data1, data2):\n",
    "   # Joins data1 and data2 vertically and returns the resulting dataframe\n",
    "   \n",
    "   # concatenate the data\n",
    "   combined_data = pd.concat([data1, data2])\n",
    "   # sort the index by ascending datetime\n",
    "   combined_data.sort_index(inplace=True)\n",
    "\n",
    "   return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_files = ['data/1946ABE.txt', 'data/1947ABE.txt']\n",
    "\n",
    "data1 = read_tidal_data(gauge_files[1])\n",
    "data2 = read_tidal_data(gauge_files[0])\n",
    "data = join_data(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_files = ['data/1946ABE.txt', 'data/1947ABE.txt']\n",
    "\n",
    "data1 = read_tidal_data(gauge_files[1])\n",
    "data2 = read_tidal_data(gauge_files[0])\n",
    "data = join_data(data1, data2)\n",
    "\n",
    "assert \"Sea Level\" in data.columns\n",
    "assert type(data.index) == pd.core.indexes.datetimes.DatetimeIndex\n",
    "assert data['Sea Level'].size == 8760*2\n",
    "\n",
    "# check sorting (we join 1947 to 1946, but expect 1946 to 1947)\n",
    "assert data.index[0] == pd.Timestamp('1946-01-01 00:00:00')\n",
    "assert data.index[-1] == pd.Timestamp('1947-12-31 23:00:00')\n",
    "\n",
    "# check you get a fail if two incompatible dfs are given\n",
    "data2.drop(columns=[\"Sea Level\",\"Time\"], inplace=True)\n",
    "data = join_data(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_year_remove_mean(year, data):\n",
    "   # Takes in dataframe data containing multiple years of data and returns a dataframe containing only data from the year year. \n",
    "   # Also removes the mean by normalising the data.\n",
    "\n",
    "   if int(year) in data.index.year:\n",
    "      data = data[data.index.year == int(year)] # ensure year is an int and not a string\n",
    "   else:\n",
    "      print(\"The data does not contain the given year!\")\n",
    "      return -1\n",
    "\n",
    "   # Normalise sea level to remove the mean as in mini course\n",
    "   data['Sea Level'] = data['Sea Level'] - data['Sea Level'].mean()\n",
    "    \n",
    "   return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not sure if I have to normalise Residual too?? As idk if this will affect the coefficient calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_files = ['data/1946ABE.txt', 'data/1947ABE.txt']\n",
    "\n",
    "data1 = read_tidal_data(gauge_files[1])\n",
    "data2 = read_tidal_data(gauge_files[0])\n",
    "data = join_data(data1, data2)\n",
    "\n",
    "year1947 = extract_single_year_remove_mean(\"1947\",data)\n",
    "assert \"Sea Level\" in year1947.columns\n",
    "assert type(year1947.index) == pd.core.indexes.datetimes.DatetimeIndex\n",
    "assert year1947['Sea Level'].size == 8760\n",
    "\n",
    "mean = np.mean(year1947['Sea Level'])\n",
    "print(mean)\n",
    "# check mean is near zero\n",
    "assert mean == pytest.approx(0)\n",
    "\n",
    "# check something sensible when a year is given that doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_section_remove_mean(start, end, data):\n",
    "    # Takes in dataframe data and returns all rows between start date and end date inclusive.\n",
    "    # Also removes the mean by normalising the data.\n",
    "\n",
    "    # Ensure the index is a DatetimeIndex\n",
    "    if not isinstance(data.index, pd.DatetimeIndex):\n",
    "        data = data.copy()\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "    \n",
    "    # Convert arguments to datetimes\n",
    "    start = pd.to_datetime(start).date()\n",
    "    end = pd.to_datetime(end).date()\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    data = data[(data.index.date >= start) & (data.index.date <= end)]\n",
    "\n",
    "    # Normalise sea level to remove the mean as in mini course\n",
    "    data['Sea Level'] = data['Sea Level'] - data['Sea Level'].mean()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_files = ['data/1946ABE.txt', 'data/1947ABE.txt']\n",
    "\n",
    "data1 = read_tidal_data(gauge_files[1])\n",
    "data2 = read_tidal_data(gauge_files[0])\n",
    "data = join_data(data1, data2)\n",
    "\n",
    "year1946_47 = extract_section_remove_mean(\"19461215\", \"19470310\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year1946_47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"Sea Level\" in year1946_47.columns\n",
    "assert type(year1946_47.index) == pd.core.indexes.datetimes.DatetimeIndex\n",
    "assert year1946_47['Sea Level'].size == 2064\n",
    "\n",
    "mean = np.mean(year1946_47['Sea Level'])\n",
    "# check mean is near zero\n",
    "assert mean == pytest.approx(0)\n",
    "\n",
    "data_segment = extract_section_remove_mean(\"19470115\", \"19470310\", data1)\n",
    "assert \"Sea Level\" in data_segment.columns\n",
    "assert type(data_segment.index) == pd.core.indexes.datetimes.DatetimeIndex\n",
    "assert data_segment['Sea Level'].size == 1320\n",
    "\n",
    "mean = np.mean(data_segment['Sea Level'])\n",
    "# check mean is near zero\n",
    "assert mean == pytest.approx(0)\n",
    "\n",
    "# check something sensible is done when dates are formatted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidal_analysis(data, constituents, start_datetime):\n",
    "   # Returns amplitude and phase for given constituents and sea level data using uptide library\n",
    "\n",
    "   # Need to remove NaNs before calculating tidal constituents\n",
    "   data = data[~data['Sea Level'].isna()]\n",
    "\n",
    "   # we create a Tides object with a list of the consituents we want.\n",
    "   tide = uptide.Tides(constituents)\n",
    "\n",
    "   # We then set out start time. All data must then be in second since this time\n",
    "   tide.set_initial_time(start_datetime)\n",
    "\n",
    "   # calculate seconds since start_datetime\n",
    "   seconds_since = (data.index.astype('int64').to_numpy()/1e9) - start_datetime.timestamp()\n",
    "\n",
    "   # Calculate amplitude and phase\n",
    "   amp, pha = uptide.harmonic_analysis(tide, data['Sea Level'].to_numpy(), seconds_since)\n",
    "   \n",
    "   return amp, pha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_segment =extract_section_remove_mean(\"19460115\", \"19470310\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_segment.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_segment = data_segment[~data_segment['Sea Level'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_segment.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_files = ['data/1946ABE.txt', 'data/1947ABE.txt']\n",
    "data1 = read_tidal_data(gauge_files[1])\n",
    "data2 = read_tidal_data(gauge_files[0])\n",
    "data = join_data(data1, data2)\n",
    "\n",
    "data_segment =extract_section_remove_mean(\"19460115\", \"19470310\", data)\n",
    "\n",
    "constituents  = ['M2', 'S2']\n",
    "tz = pytz.timezone(\"utc\")\n",
    "start_datetime = datetime.datetime(1946,1,15,0,0,0, tzinfo=tz)\n",
    "amp,pha = tidal_analysis(data_segment, constituents, start_datetime)\n",
    "print(amp, pha)\n",
    "# for Aberdeen, the M2 and S2 amps are 1.307 and 0.441\n",
    "assert amp[0] == pytest.approx(1.307,abs=0.1)\n",
    "assert amp[1] == pytest.approx(0.441,abs=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FD = pd.read_csv('h333.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only other idea I have is to calculate slope for each day in the dataset and then average over all values to get the whole year. Would have to average the p value as well so doesn't really make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sea_level_rise(data):\n",
    "   # Uses linear regression to calculate sea level rise in metres per year. Returns the slope (sea level rise) and p value.\n",
    "\n",
    "   # Remove NaNs\n",
    "   data = data[~data['Sea Level'].isna()]\n",
    "   \n",
    "   # Convert dates to numbers for the regression\n",
    "   x = date2num(data.index)\n",
    "   y = data['Sea Level']\n",
    "   \n",
    "   # use scipy.stats.linregress\n",
    "   slope, intercept, r, p, se = linregress(x, y)\n",
    "\n",
    "   return slope, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_files = ['data/1946ABE.txt', 'data/1947ABE.txt']\n",
    "data1 = read_tidal_data(gauge_files[1])\n",
    "data2 = read_tidal_data(gauge_files[0])\n",
    "data = join_data(data1, data2)\n",
    "\n",
    "slope, p_value = sea_level_rise(data)\n",
    "print(slope, p_value)\n",
    "assert slope == pytest.approx(2.94e-05,abs=1e-7)\n",
    "assert p_value == pytest.approx(0.427,abs=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.8479573538948342e-05+1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_contiguous_data(data):\n",
    "    # Returns the start date and end date of the longest contiguous period of data (no missing timestamps or values) as well as the length of this period.\n",
    "\n",
    "    # Ensure the index is sorted\n",
    "    data = data.sort_index()\n",
    "    \n",
    "    # Create a boolean mask for valid rows (no NaNs and hourly frequency maintained)\n",
    "    # Step 1: Identify where the data is valid (not NaN)\n",
    "    valid = data['Sea Level'].notna()\n",
    "    #print(valid)\n",
    "    # Step 2: Identify time gaps larger than 1 hour\n",
    "    time_diffs = data.index.to_series().diff().gt(pd.Timedelta(hours=1))\n",
    "    time_diffs.iloc[0] = False  # First entry has no diff\n",
    "    #print(time_diffs)\n",
    "    # Combine both masks: invalid if NaN or gap in time\n",
    "    invalid = ~valid | time_diffs\n",
    "    #print(invalid)\n",
    "    # Assign a group number that increases whenever a break (invalid) occurs\n",
    "    group_id = (~invalid).cumsum() * (~invalid)\n",
    "    #print(group_id)\n",
    "    # Count the size of each valid group\n",
    "    group_sizes = group_id[group_id > 0].value_counts()\n",
    "    print(group_sizes.max())\n",
    "    if group_sizes.empty:\n",
    "        return None, None, 0  # No valid data\n",
    "\n",
    "    longest_group = group_sizes.idxmax()\n",
    "    max_length = group_sizes.max()\n",
    "\n",
    "    # Get timestamps of the longest group\n",
    "    period_data = data[group_id == longest_group]\n",
    "    start = period_data.index[0]\n",
    "    end = period_data.index[-1]\n",
    "\n",
    "    return start, end, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_files = ['data/1946ABE.txt', 'data/1947ABE.txt']\n",
    "data1 = read_tidal_data(gauge_files[1])\n",
    "data2 = read_tidal_data(gauge_files[0])\n",
    "data = join_data(data1, data2)\n",
    "\n",
    "st, end, length = find_longest_contiguous_data(data)\n",
    "print(st, end, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_contiguous_data(data):\n",
    "    # Returns the start date and end date of the longest contiguous period of data (no missing timestamps or values) as well as the length of this period.\n",
    "\n",
    "    # Ensure the index is sorted and is a DatetimeIndex\n",
    "    data = data.sort_index()\n",
    "    if not isinstance(data.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame index must be a DatetimeIndex\")\n",
    "\n",
    "    # Step 1: Identify invalid rows (NaN or time gaps)\n",
    "    valid = data['Sea Level'].notna()\n",
    "    time_gap = data.index.to_series().diff().gt(pd.Timedelta(hours=1))\n",
    "    time_gap.iloc[0] = False\n",
    "    breaks = (~valid) | time_gap\n",
    "\n",
    "    # Step 2: Mark groups - a new group starts after a break\n",
    "    group_id = breaks.cumsum()\n",
    "\n",
    "    # Step 3: Group by group_id and find the longest one\n",
    "    data_valid = data[~breaks]\n",
    "    if data_valid.empty:\n",
    "        return None, None, 0\n",
    "\n",
    "    groups = data_valid.groupby(group_id)\n",
    "    longest_group = max(groups, key=lambda g: len(g[1]))\n",
    "\n",
    "    start = longest_group[1].index[0]\n",
    "    end = longest_group[1].index[-1]\n",
    "    length = len(longest_group[1])\n",
    "\n",
    "    return start, end, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_files = ['data/1946ABE.txt', 'data/1947ABE.txt']\n",
    "data1 = read_tidal_data(gauge_files[1])\n",
    "data2 = read_tidal_data(gauge_files[0])\n",
    "data = join_data(data1, data2)\n",
    "\n",
    "st, end, length = find_longest_contiguous_data(data)\n",
    "print(st, end, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_folder(folder_name):\n",
    "    # Reads in all .txt files in folder_name and joins them into one text file. This file is returned.\n",
    "\n",
    "    # Get all txt files in the folder\n",
    "    txt_files = [f for f in os.listdir(folder_name) if fnmatch.fnmatch(f, '*.txt')]\n",
    "\n",
    "    data = read_tidal_data(folder_name + '/' + txt_files[0])\n",
    "    # Concatenate all the txt files into one\n",
    "    for file in txt_files[1:]:\n",
    "        data_tmp = read_tidal_data(folder_name + '/' + file)\n",
    "        data = join_data(data, data_tmp)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_folder('data/whitby')\n",
    "\n",
    "print(len(data))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"The program should print the tidal data, the sea-level rise and the longest contiguous period of data (i.e. without any missing data) from the data loaded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st, end, length = get_longest_contiguous_data(data)\n",
    "print(\"Start date: \", st, \"End date: \", end, \"Longest contiguouos period: \", length)\n",
    "\n",
    "slope, p_value = sea_level_rise(data)\n",
    "print(\"Sea level rise rate (slope in metres per unit time): \", slope, \"p-value: \", p)\n",
    "\n",
    "constituents  = ['M2', 'S2']\n",
    "start_datetime = data.index[0]\n",
    "amp, pha = tidal_analysis(data, constituents, start_datetime)\n",
    "print(\"Amplitude of M2, S2: \", amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
